{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Harman eNova Speech Recognition Model Package from AWS Marketplace \n",
    "\n",
    "\n",
    "Harman eNova Speech Recognition offers real-time transcription, built to scale for the enterprise. Optimized for CPU instances so you can focus on getting the insights you need from your voice data with minimal hardware requirement.\n",
    "\n",
    "This sample notebook shows you how to deploy using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "    \n",
    "- The input audio data should consist of complete audio files, rather than raw PCM data.\n",
    "- The input audio sample should be 16000 or above. \n",
    "- The maximum audio file size for realtime inference is 20MB and for batch transform 50MB each file.\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page.\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is sample ARN please replace with the subcribed ARN.\n",
    "model_package_arn = \"arn:aws:sagemaker:us-east-1:822940408628:model-package/marketplace-speech-recognition-1-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import json \n",
    "import uuid\n",
    "import boto3\n",
    "import base64\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lifeware-marketplace-speech-recognition-model-ep\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "accept = \"text/plain\"\n",
    "real_time_inference_instance_type = 'ml.c5.xlarge'\n",
    "batch_transform_inference_instance_type = 'ml.m4.16xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Sample Input files https://github.com/HDTS-user/lifeware-speech-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we pull a sample input file for testing the model. For example we are refering to a .flac file which is local to \n",
    "#this notebook.\n",
    "sample_audio_file = 'sample-audio-files/8297-275155-0032.flac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now encode the audio in base64, this is the input format supported for inferencing with endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample_audio_file, mode='rb') as file:\n",
    "        fileContent = file.read()\n",
    "encodeddata = base64.b64encode(fileContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invoke the endpoint to get real time inference. For this example we are using \"transcribe\" task. Simillary same endpoint can be used for generating srt files.\n",
    "Avialable Tasks: \"transcribe\", \"transcribe_srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_payload['wv_data'] =  encodeddata.decode('utf-8')\n",
    "json_payload['task'] =\"transcribe\"\n",
    "json_payload['lang'] =\"en\"\n",
    "data = json.dumps(json_payload)\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "res = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    Body=data, \n",
    "    ContentType=content_type,  \n",
    "    Accept=accept,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " While he was walking up and down the platform, with a mind doubly distressed by anxiety about his brother and anxiety about Sydney, the train from London came in.\n"
     ]
    }
   ],
   "source": [
    "print(res['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate srt output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:05,480\n",
      "While he was walking up and down the platform with a mind doubly distressed by anxiety about\n",
      "2\n",
      "00:00:05,480 --> 00:00:35,440\n",
      "his brother and anxiety about Sydney, the train from London came in.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_payload['wv_data'] =  encodeddata.decode('utf-8')\n",
    "json_payload['task'] =\"transcribe_srt\"\n",
    "json_payload['lang'] =\"en\"\n",
    "data = json.dumps(json_payload)\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "res = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    Body=data,\n",
    "    ContentType=content_type,  \n",
    "    Accept=accept,  \n",
    ")\n",
    "\n",
    "print(res['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input uploaded..\n",
      "sample-audio/\n",
      "sample-audio/8297-275155-0031.flac\n",
      "sample-audio/8297-275155-0032.flac\n"
     ]
    }
   ],
   "source": [
    "# upload the batch-transform job input files to S3\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Set the local path of the folder where sample audio files are kept\n",
    "sample_audio_folder_path = 'sample-audio-files'\n",
    "\n",
    "# Set the name of your S3 bucket and the name of the folder you want to create\n",
    "bucket_name = 'sample-s3-bucket-for-speech-recog-test4'\n",
    "input_folder_name = 'sample-audio'\n",
    "\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create the S3 bucket\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Create the folder in the S3 bucket\n",
    "s3.put_object(Bucket=bucket_name, Key=input_folder_name + '/')\n",
    "\n",
    "# Upload a file to the folder in the S3 bucket\n",
    "for dirName, subdirList, fileList in os.walk(sample_audio_folder_path):\n",
    "    for fname in fileList:\n",
    "        response = s3.upload_file(dirName+'/'+fname, bucket_name, input_folder_name + '/' + fname)\n",
    "print(\"Transform input uploaded..\")\n",
    "#Display the content of S3 Bucket\n",
    "# List the objects in the bucket\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "for obj in response['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:822940408628:model/lifeware-marketplace-speech-recognition-model-batch-inference\n"
     ]
    }
   ],
   "source": [
    "ModelName = \"lifeware-marketplace-speech-recognition-model-batch-inference\"\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "cm_res_end = sm_boto3.create_model(\n",
    "    ModelName=ModelName,  # name the of the model does not need to be the same as the image repob\n",
    "     Containers=[\n",
    "        {            \n",
    "            'ModelPackageName': model_package_arn\n",
    "        },\n",
    "    ],    \n",
    "    ExecutionRoleArn=get_execution_role(),\n",
    "    EnableNetworkIsolation=False,\n",
    ")\n",
    "print(\"Model Arn: \" + cm_res_end['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformjobname= \"lifeware-marketplace-speech-recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TransformJobArn': 'arn:aws:sagemaker:us-east-1:822940408628:transform-job/lifeware-marketplace-speech-recognition-202212151428', 'ResponseMetadata': {'RequestId': '9a97f4c1-6ff2-4bed-857a-bd4ac9297d07', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9a97f4c1-6ff2-4bed-857a-bd4ac9297d07', 'content-type': 'application/x-amz-json-1.1', 'content-length': '129', 'date': 'Thu, 15 Dec 2022 14:28:10 GMT'}, 'RetryAttempts': 0}}\n",
      "Output is available on following path: s3://sample-s3-bucket-for-speech-recog-test2/output/\n"
     ]
    }
   ],
   "source": [
    "# Run the batch-transform job\n",
    "#optionl step to make uniue job name\n",
    "transformjobname = transformjobname + time.strftime(\"-%Y%m%d%H%M\", time.gmtime())\n",
    "\n",
    "response_create_job = sm_boto3.create_transform_job(\n",
    "    TransformJobName=transformjobname,\n",
    "    ModelName=ModelName,\n",
    "    MaxConcurrentTransforms=1,\n",
    "    #InstanceCount=1\n",
    "    MaxPayloadInMB=50,   \n",
    "    Environment={\n",
    "        'TASK': 'transcribe_srt',  # Creating srt output, it can be simple transcribe text also.\n",
    "        'LANG': 'en'\n",
    "    },\n",
    "    TransformInput={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {                \n",
    "                'S3DataType': 'S3Prefix', \n",
    "                'S3Uri': \"s3://\"+bucket_name+\"/\"    # Input Folder\n",
    "            }\n",
    "        },\n",
    "        'ContentType':\"audio/x-wav\"\n",
    "    },\n",
    "    TransformOutput={\n",
    "        'S3OutputPath': \"s3://\"+bucket_name+\"/output/\", #output folder\n",
    "        'Accept': 'application/json',\n",
    "        'AssembleWith': 'Line',        \n",
    "    },\n",
    "    TransformResources={\n",
    "        'InstanceType': batch_transform_inference_instance_type,\n",
    "        'InstanceCount': 1      \n",
    "    }, \n",
    ")\n",
    "print(response_create_job)\n",
    "print(\"Output is available on following path:\",s3audiooutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a6a99cd5-356f-4f8e-9b86-4f28912239fd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a6a99cd5-356f-4f8e-9b86-4f28912239fd',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 15 Dec 2022 13:34:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_boto3.delete_model(ModelName=ModelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
